<launch>
    <arg name="drone_name" default="tello" />

    <!-- Code Llama -->
    <!-- <arg name="ollama_model" default="codellama:7b" doc="The Ollama model to use"/> -->
    <!-- <arg name="ollama_model" default="codellama:13b" doc="The Ollama model to use"/> -->

    <!-- Llama3 -->
    <!-- <arg name="ollama_model" default="llama3.1:8b" doc="The Ollama model to use"/> -->

    <!-- DeepSeek -->
    <!-- <arg name="ollama_model" default="deepseek-r1:1.5b" doc="The Ollama model to use"/> -->
    <!-- <arg name="ollama_model" default="deepseek-r1:8b" doc="The Ollama model to use"/> -->

    <!-- Qwen3 -->
    <!-- <arg name="ollama_model" default="qwen3:4b" doc="The Ollama model to use"/> -->
    <!-- <arg name="ollama_model" default="qwen3:8b" doc="The Ollama model to use"/> -->
    <arg name="ollama_model" default="qwen3:14b" doc="The Ollama model to use"/>

    <!-- System prompt -->
    <!-- <arg name="tools_config_path" default="$(find tello_llm_ros)/config/llm_tools.json" /> -->
    <arg name="tools_config_path" default="$(find tello_llm_ros)/config/pure_text_system_prompt-EN.txt" />

    <node name="tello_llm_controller" pkg="tello_llm_ros" type="llm_node.py" output="screen">
        <param name="drone_name" value="$(arg drone_name)" />
        <param name="ollama_model" value="$(arg ollama_model)" />
        <param name="tools_config_path" value="$(arg tools_file)" />
    </node>
</launch>
<launch>

    <!-- code llama -->
    <!-- <arg name="ollama_model" default="codellama:7b" doc="The Ollama model to use"/> -->

    <!-- Llama3 -->
    <!-- <arg name="ollama_model" default="llama3.1:8b" doc="The Ollama model to use"/> -->

    <!-- DeepSeek -->
    <!-- <arg name="ollama_model" default="deepseek-r1:1.5b" doc="The Ollama model to use"/> -->
    <!-- <arg name="ollama_model" default="deepseek-r1:8b" doc="The Ollama model to use"/> -->

    <!-- Qwen3 -->
    <arg name="ollama_model" default="qwen3:4b" doc="The Ollama model to use"/>
    <!-- <arg name="ollama_model" default="qwen3:8b" doc="The Ollama model to use"/> -->
    <!-- <arg name="ollama_model" default="qwen3:14b" doc="The Ollama model to use"/> -->


    <arg name="tools_config_path" default="$(find tello_llm_ros)/config/llm_tools.json" />

    <node name="tello_llm_controller" pkg="tello_llm_ros" type="test_llm_offline.py" output="screen">
        <param name="ollama_model" value="$(arg ollama_model)" />
        <param name="tools_config_path" value="$(arg tools_config_path)" />
    </node>

</launch>
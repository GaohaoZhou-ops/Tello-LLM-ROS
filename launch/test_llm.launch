<launch>

    <!-- Code Llama -->
    <!-- <arg name="ollama_model" default="codellama:7b" doc="The Ollama model to use"/> -->
    <!-- <arg name="ollama_model" default="codellama:13b" doc="The Ollama model to use"/> -->

    <!-- Llama3 -->
    <!-- <arg name="ollama_model" default="llama3.1:8b" doc="The Ollama model to use"/> -->

    <!-- DeepSeek -->
    <arg name="ollama_model" default="deepseek-r1:8b" doc="The Ollama model to use"/>
    <!-- <arg name="ollama_model" default="deepseek-r1:14b" doc="The Ollama model to use"/> -->

    <!-- Qwen3 -->
    <!-- <arg name="ollama_model" default="qwen3:4b" doc="The Ollama model to use"/> -->
    <!-- <arg name="ollama_model" default="qwen3:8b" doc="The Ollama model to use"/> -->
    <!-- <arg name="ollama_model" default="qwen3:14b" doc="The Ollama model to use"/> -->

    <!-- System prompt -->
    <arg name="common_system_prompt_file" default="$(find tello_llm_ros)/config/common_system_prompt-EN.txt" />
    <arg name="tools_description_file" default="$(find tello_llm_ros)/config/pure_text_tools_description-EN.txt" />


    <node name="tello_llm_controller" pkg="tello_llm_ros" type="test_llm_offline.py" output="screen">
        <param name="ollama_model" value="$(arg ollama_model)" />
        <param name="common_system_prompt_file" value="$(arg common_system_prompt_file)" />
        <param name="tools_description_file" value="$(arg tools_description_file)" />
    </node>

</launch>
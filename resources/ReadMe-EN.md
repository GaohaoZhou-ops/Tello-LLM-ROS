# Tello LLM ROS

This repository implements LLM for controlling the Tello drone within the ROS framework. It takes natural language commands as input, combines them with prompts and tools definitions, and outputs drone control commands. Currently, multiple debugging combinations are supported:

|Model| Drone | Support |
|---|---|---|
| Ollama Local | Simulate & Real | ‚úÖ |
| deepseek-chat | Simulate & Real | ‚úÖ |
| gemini-2.5-flash | Simulate & Real | ‚úÖ |
| LAN Server | Simulate & Real | ‚úÖ |
| gpt-40 Online | Simulate & Real | ‚úÖ |
| Ernie Online | Simulate & Real | ‚úÖ |

We also tested some local and online models. Please refer to the `Benchmarks` section of this document for more details.

If you want to control the drone using an XBox controller, you can refer to our other two open source projects:

* [XBox Controller Reader](https://github.com/GaohaoZhou-ops/XboxControllerReader): Independent of ROS, it can be started remotely.
* [xbox_controller_pkg](https://github.com/GaohaoZhou-ops/xbox_controller_pkg): A ROS node package.

If you want to deploy a local server inference client within the same LAN, follow the instructions in this [ReadMe](../LAN-Server/ReadMe.md) file.

# üéâ News!

### Tuesday, August 19, 2025

* Added photo and video capture capabilities;
* Unified client code supporting OpenAI protocol calls;

### Monday, August 18, 2025

Added support for Gemini, local LAN servers, and OpenAI calls;

### Sunday, August 17, 2025

We've extensively refactored this project and now support both local Ollama and online DeepSeek API calls. Clients for more common models are in development, so stay tuned.

### Saturday, August 15, 2025

Unified prompt generation for model testing and application;

-----
## 1. Installation and Deployment ü™§

Whether or not you use a local model, you'll need to configure and deploy the basic environment. Follow the steps below to complete this.

### 1.1 Installing Basic Dependencies
Before running, you may need to install the following dependencies, including but not limited to:

```bash
$ conda install libffi==3.3
```

### 1.2 Creating a Conda Environment

```bash
$ conda create -n tello python=3.8
$ conda activate tello
$ pip install -r requirements.txt
```

### 1.3 Compiling from Source

Enter your project and pull down the source code. Here, we assume it's `tello_ws`:

```bash
$ cd tello_ws/src
$ git clone https://github.com/GaohaoZhou-ops/Tello-LLM-ROS.git
```

Once the source code is pulled down, you can compile:

```bash
$ cd tello_ws
$ catkin_make
```

### 1.4 Google Gemini

If you plan to use Google Gemini If you are using an online model and your conda environment is Python 3.9+, you will also need to install gcloud CIL by referring to the following link. However, please note that this step should be installed in the conda environment we created:

* Google Cloud CIL: [https://cloud.google.com/sdk/docs/install?hl=zh-cn#linux](https://cloud.google.com/sdk/docs/install?hl=zh-cn#linux)

After installation, execute the following command locally and follow the prompts to log in to Google Cloud:

```bash
$ source ~/.bashrc
$ gcloud auth application-default login
```

Then rename the `llm_models/gemini_client_for_py39+.py` file to `gemini_client.py` and overwrite it.

If your conda environment is the same as our test environment, you can use it directly.

-----
## 2. How to Use üíª

This project supports both local and online model invocation. This chapter explains how to use the entire project.

### 2.1 Configuring the Prompt Term Tool üîî
Whether you choose to invoke the model locally or online, we recommend carefully reviewing the prompt terms and modifying them, if necessary, to better suit your current task. The project's prompt files for the language model are stored in the `config` directory. The `prompts` directory provides system prompts in multiple languages, as well as plain text prompt files. This design is based on the following considerations:

1. Testing has found that using plain text to describe tools significantly improves accuracy for the local model. This is likely due to the extra tokens generated by parsing the JSON file, which is not conducive to long-term memory for small-parameter models.
2. We also recommend using plain text prompts when calling the online model, as this reduces API token consumption.

```bash
.
‚îú‚îÄ‚îÄ prompts
‚îÇ ‚îú‚îÄ‚îÄ common_system_prompt-CN.txt # Common system prompts
‚îÇ ‚îú‚îÄ‚îÄ common_system_prompt-EN.txt
‚îÇ ‚îú‚îÄ‚îÄ pure_text_tools_description-CN.txt # Tools description
‚îÇ ‚îî‚îÄ‚îÄ pure_text_tools_description-EN.txt
‚îú‚îÄ‚îÄ test_cases.json # Model test cases
‚îî‚îÄ‚îÄ tools.json # Tools description file
```

You can modify these files, but please note that if you add or modify a tool, you must also modify the corresponding tool code in the source code.

### 2.2 Model Performance Testing üåü

We strongly recommend running a simple test of your selected model before using it on a real device or in simulation. The `config/test_cases.json` file describes the test cases. You can add or delete test cases based on your task requirements.

Several parameters in the `launch/llm_service.launch` file determine the model used in the test environment:

```xml
<arg name="model_type" default="deepseek"/>
<arg name="model_name" default="deepseek-chat"/>
<arg name="api_key" default="Your online mode API Key" />
```

* `model_type`: Model type. The current version only supports `ollama` and `deepseek`.

* `model_name`: Model name.
* `api_key`: If you are using the local ollama model, this parameter can be left blank.

For information about model types, model names, and URLs, refer to the `launch/supported_model_config` file. This file contains the configuration used during testing.

Then run the model performance test using the following command:

```bash
$ cd tello_llm_ros
$ source devel/setup.bash
$ roslaunch tello_llm_ros llm_test.launch
```

![llm_test](./images/llm_test.png)

### 2.2 Joint Debugging

Once you've determined the model to use, you can begin the joint debugging phase. The project provides multiple joint debugging methods.

#### Local Ollama

If you plan to use a local model, you must download the model before running. We recommend using the `llama3.1:8b 4.9GB` model because it strikes a balance between accuracy and output speed in our benchmarks, although its accuracy is still lower than that of the online model.

```bash
$ ollama pull llama3.1:8b
```

#### Simulation Control

If you have special needs, such as debugging the proper functioning of the visual SLAM system on the server, you can simply start a drone simulation node. Although this node will not generate any images, it will continuously publish a solid black image.

The `use_sim` parameter in the following script determines whether to run simulation. This allows you to control the drone's takeoff, landing, and other actions through topics:

```bash
$ roslaunch tello_llm_ros tello.launch
```

![tello_sim](./images/tello_sim.png)

#### Simulation + LLM

Before conducting real-device experiments, it is recommended to complete the LLM debugging in simulation. After running the following scripts, you can send commands to the LLM in the terminal:

```bash
# Terminal 1
$ roslaunch tello_llm_ros tello.launch

# Terminal 2
$ unset all_proxy
$ unset ALL_PROXY
$ roslaunch tello_llm_ros control_node.launch
```

After these nodes are successfully started, you can open a new terminal and start the Simple Client. Entering `quit` terminates the program:

```bash
$ rosrun tello_llm_ros simple_llm_client.py
```

The following example calls the online `DeepSeek-Chat` model and gives two commands:

* `go back to seen what is`;
* `take a few steps forward to seen more clearly`;

![interface_demo](./images/interface_demo.png)

#### Real Device + LLM

After debugging the entire process in simulation, you can start the real device for joint debugging. The steps are the same as above, but remember to change the `use_sim` parameter in the `launch/tello.launch` file to `false`.

-----
# Benchmarks üèÉ

## Local Model Testing
Currently, we've only conducted experiments on the Nvidia Jetson Orin 64GB DK. We will attempt to test on a wider range of hardware devices in the future. The system and library information for the experimental environment are as follows:

![jetson_release](./images/jetson_release.png)

Based on this, we evaluated the performance of several different local models. For test samples, see the `_define_test_cases` function in the `src/tello_llm_ros/scripts/test_llm_offline.py` script:

|Model|Size|Accuracy|Average Response Time s|Average Generation Rate tokens/s|
|--|--|--|--|--|
| codellama:7b | 3.8 GB | 35.00% | 1.58 | 433.53 |
| codellama:13b | 4.7 GB | 55.00% | 3.44 | 191.98 |
| codellama:34b | 19.0 GB | 50.00% | 7.84 | 84.70 |
| llama3.1:8b | 4.9 GB | 60.00% | 2.04 | 257.65 |
| llama3-groq-tool-use:8b | 4.7 GB | 50.00% | 2.03 | 261.59 |
| qwen3:4b | 2.5 GB | 50.00% | 80.61 | 32.65 |
| qwen3:8b | 5.2 GB | 65.00% | 35.19 | 34.17 |
| qwen3:14b | 9.3 GB | 65.00% | 45.806 | 23.50 |
| deepseek-coder-v2:16b | 8.9 GB | 60.00% | 1.56 | 376.31 |
| gpt-oss:20b | 13 GB | 70.00% | 24.05 | 33.81 |

Our preliminary experiments yielded the following conclusions:

1. Most local model test cases failed due to the addition of the `takeoff` and `land` commands.

2. For local models with small parameter counts, using plain text system prompts yields higher success rates than using JSON tool descriptions.

3. Local models tend to split a single action into multiple commands, which may be related to the system prompts. For example, for "rotate 180 degrees," the model will output the command "rotate 90 degrees" twice.

4. Code-type local models respond much faster on a single task than general-purpose models.

To minimize overall system response time, we implement direct calls for some explicit commands, such as `takeoff`. These commands are not fed into the model for inference. You can also add more direct commands by modifying the `direct_triggers` field in the `config/llm_tools.json` file as follows. The `takeoff`, `take off`, and `launch` commands can all be directly responded to:

```json
{
"name": "takeoff",
"description": "Initiates the drone's automatic takeoff sequence...",
"direct_triggers": [
"takeoff",
"take off",
"launch"
],
"parameters": [],
"ros_service": "/takeoff",
"service_type": "Trigger"
},
```

## Online Model Testing üåê

For online models, we have currently only tested the `DeepSeek-Chat` model. Testing of `ChatGPT` and `Gemini` is in progress.

|Model|Accuracy|Average Response Time (s)|
|--|--|--|
| deepSeek-chat | 85.00% | 4.84 |
| gemini-2.5-flash | 95.00% | 5.12 |
| gpt-4o | 95.00% | 4.44 |
| ernie-4.0-turbo-8k | 95.00% | 2.82 |
| qwen-plus | 85.00% | 1.10 |

